#!/bin/bash

echo "$0: BEGIN"
source ../tmp/venv/bin/activate

# Expected file structure (from .install):
#
# ../tmp/native_client/
#      *
# ../keep/
#      source_model/
#          checkpoint
#          model.meta
#          model.data
#          model.index

# Created by filter_train_dev_test.py (this script)
#
# ../keep/
#     target_lang/
#         cv_valid_dev.csv
#         cv_valid_test.csv
#         cv_valid_train.csv

# Created by DeepSpeech.py (this script):
#
# ../keep/
#     target_lang/
#         source_model/
#             run/
#               RESULTS.json
#               summaries/
#               model/
#               ckpt/
#

target_lang='cv'
source_model='en'

data="../keep/${target_lang}"
mkdir $data

python3 filter_train_dev_test.py "${target_lang}" "${data}"
python3 util/check_characters.py "${data}/*.csv"


for numLayersDropped in 1 2 3 4 5; do

    exp="../keep/${target_lang}/${source_model}/${num_layers_dropped}"
    
    python3 -u DeepSpeech.py \
	    --export_dir "${exp}/model/" \
	    --summary_dir "${exp}/summaries/" \
	    --checkpoint_dir "${exp}/ckpt/" \
	    --test_output_file "${exp}/RESULTS.json" \
	    --drop_source_layers "${numLayersDropped}" \
	    --source_model_checkpoint_dir "../keep/${source_model}" \
	    --train_files "${data}/cv_valid_train.csv"\
	    --dev_files "${data}/cv_valid_dev.csv" \
	    --test_files "${data}/cv_valid_test.csv" \
	    --n_hidden 2048 \
	    --epoch -500 \
	    --train_batch_size 24 \
	    --dev_batch_size 48 \
	    --test_batch_size 48 \
	    --learning_rate 0.0001 \
	    --dropout_rate 0.2 \
	    --display_step 0 \
	    --validation_step 1 \
	    --decoder_library_path "../tmp/native_client/libctc_decoder_with_kenlm.so"
    
done

echo "$0: END"
